{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import Library\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution3D, MaxPooling3D\n",
    "from keras.optimizers import Adam,SGD, RMSprop\n",
    "from keras.utils import np_utils, generic_utils\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set up label and  config\n",
    "classes = ['Swiping Left', 'Swiping Right', 'Swiping Down', 'Swiping Up', 'Zooming In', 'Zooming Out', 'No Gesture' ]\n",
    "img_rows,img_cols,img_depth = 84, 84, 18\n",
    "step_sample = 2\n",
    "\n",
    "# data folder\n",
    "train_folder = 'dataset\\\\train\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 18, 84, 84)\n",
      "load Swiping Left data completed!\n",
      "(2000, 18, 84, 84)\n",
      "load Swiping Right data completed!\n",
      "(3000, 18, 84, 84)\n",
      "load Swiping Down data completed!\n",
      "(4000, 18, 84, 84)\n",
      "load Swiping Up data completed!\n",
      "(5000, 18, 84, 84)\n",
      "load Zooming In data completed!\n",
      "(6000, 18, 84, 84)\n",
      "load Zooming Out data completed!\n",
      "(7000, 18, 84, 84)\n",
      "load No Gesture data completed!\n"
     ]
    }
   ],
   "source": [
    "# Load training data\n",
    "X_tr = []     # variable to store entire dataset\n",
    "\n",
    "#Reading data from each class\n",
    "for class_name in classes:    \n",
    "    class_folder = os.path.join(train_folder, class_name )\n",
    "    listing = os.listdir(class_folder)\n",
    "    for vid in listing:\n",
    "        frames = []\n",
    "        # set number of necessary frames\n",
    "        num_frames_necessary = img_depth * step_sample \n",
    "        vid = os.path.join(class_folder, vid)\n",
    "        frame_names = os.listdir(vid)\n",
    "        frame_names = list(sorted(frame_names))\n",
    "    #     print(frame_names)\n",
    "        num_frames = len(frame_names)\n",
    "        # pick frames\n",
    "        offset = 0\n",
    "        if num_frames_necessary > num_frames:\n",
    "            # Pad last frame if video is shorter than necessary\n",
    "            frame_names += [frame_names[-1]] * \\\n",
    "                (num_frames_necessary - num_frames)        \n",
    "        elif num_frames_necessary < num_frames:\n",
    "            # If there are more frames, then sample starting offset.\n",
    "            diff = (num_frames - num_frames_necessary)\n",
    "            offset = np.random.randint(0, diff)\n",
    "        frame_names = frame_names[offset:num_frames_necessary + offset:2]      \n",
    "        for name in frame_names:\n",
    "            name = os.path.join(vid,name)\n",
    "    #         print(name)\n",
    "            frame= cv2.imread(name)\n",
    "            frame=cv2.resize(frame,(img_rows,img_cols),interpolation=cv2.INTER_AREA)\n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            frames.append(gray)      \n",
    "        ipt = np.array(frames)     \n",
    "        X_tr.append(ipt)\n",
    "    print(np.array(X_tr).shape)\n",
    "    print('load {} data completed!'.format(class_name))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7000, 18, 84, 84)\n"
     ]
    }
   ],
   "source": [
    "X_tr = np.array(X_tr)\n",
    "print(X_tr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7000, 18, 84, 84)\n"
     ]
    }
   ],
   "source": [
    "# Assign Label to each class for train set\n",
    "num_samples = X_tr.shape[0]\n",
    "label=np.ones((num_samples,),dtype = int)\n",
    "label[0:1000]= 0         # swiping left\n",
    "label[100:2000] = 1      # swiping right\n",
    "label[2000:3000] = 2     # swiping down\n",
    "label[3000:4000] = 3     # swiping up\n",
    "label[4000:5000] = 4     # zooming in\n",
    "label[5000:6000] = 5     # zooming out\n",
    "label[6000:7000] = 6     # No gestures\n",
    "\n",
    "\n",
    "X_train = np.array(X_tr) \n",
    "y_train = label\n",
    "\n",
    "print(X_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7000, 18, 84, 84, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.expand_dims(X_train, axis=-1)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre-processing\n",
    "X_train = X_train .astype('float32')\n",
    "X_train  -= np.mean(X_train )\n",
    "X_train  /= np.max(X_train )\n",
    "\n",
    "nb_classes = 7\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers.convolutional import Conv3D, MaxPooling3D\n",
    "\"\"\"See: 'https://arxiv.org/pdf/1412.0767.pdf' \"\"\"\n",
    "# Tunable parameters\n",
    "kernel_size = (3, 3, 3)\n",
    "strides = (1, 1, 1)\n",
    "extra_conv_blocks = 1\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Conv Block 1\n",
    "model.add(Conv3D(32, kernel_size, strides=strides, activation='relu',\n",
    "                 padding='same', input_shape= (18,84,84,1)))\n",
    "model.add(MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2)))\n",
    "\n",
    "# Conv Block 2\n",
    "model.add(Conv3D(64, kernel_size, strides=strides, activation='relu',\n",
    "                 padding='same'))\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2)))\n",
    "\n",
    "# Conv Block 3\n",
    "model.add(Conv3D(128, kernel_size, strides=strides, activation='relu',\n",
    "                 padding='same'))\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2)))\n",
    "\n",
    "# Conv Block 4\n",
    "model.add(Conv3D(128, kernel_size, strides=strides, activation='relu',\n",
    "                 padding='same'))\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2)))\n",
    "\n",
    "# Dense Block\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(nb_classes, activation='softmax'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_1 (Conv3D)            (None, 18, 84, 84, 32)    896       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3 (None, 18, 42, 42, 32)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_2 (Conv3D)            (None, 18, 42, 42, 64)    55360     \n",
      "_________________________________________________________________\n",
      "max_pooling3d_2 (MaxPooling3 (None, 9, 21, 21, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_3 (Conv3D)            (None, 9, 21, 21, 128)    221312    \n",
      "_________________________________________________________________\n",
      "max_pooling3d_3 (MaxPooling3 (None, 4, 10, 10, 128)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_4 (Conv3D)            (None, 4, 10, 10, 128)    442496    \n",
      "_________________________________________________________________\n",
      "max_pooling3d_4 (MaxPooling3 (None, 2, 5, 5, 128)      0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 6400)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               1638656   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 7)                 1799      \n",
      "=================================================================\n",
      "Total params: 2,426,311\n",
      "Trainable params: 2,426,311\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimizer = Adam(lr=0.001)\n",
    "model.compile(loss='categorical_crossentropy', optimizer= optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train_new, X_val_new, y_train_new, y_val_new = train_test_split(X_train, Y_train, test_size=0.2, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "filepath = \"handgesture.weights.best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose =1, save_best_only = True, mode ='max')\n",
    "callbacks_list= [checkpoint]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5600 samples, validate on 1400 samples\n",
      "Epoch 1/10\n",
      " - 50s - loss: 1.1672 - acc: 0.5534 - val_loss: 0.8077 - val_acc: 0.7057\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.70571, saving model to handgesture.weights.best.hdf5\n",
      "Epoch 2/10\n",
      " - 47s - loss: 0.7189 - acc: 0.7375 - val_loss: 0.5497 - val_acc: 0.8257\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.70571 to 0.82571, saving model to handgesture.weights.best.hdf5\n",
      "Epoch 3/10\n",
      " - 47s - loss: 0.5533 - acc: 0.8130 - val_loss: 0.5215 - val_acc: 0.8171\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.82571\n",
      "Epoch 4/10\n",
      " - 47s - loss: 0.4344 - acc: 0.8554 - val_loss: 0.4590 - val_acc: 0.8557\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.82571 to 0.85571, saving model to handgesture.weights.best.hdf5\n",
      "Epoch 5/10\n",
      " - 47s - loss: 0.3618 - acc: 0.8827 - val_loss: 0.4480 - val_acc: 0.8621\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.85571 to 0.86214, saving model to handgesture.weights.best.hdf5\n",
      "Epoch 6/10\n",
      " - 47s - loss: 0.2950 - acc: 0.9075 - val_loss: 0.4357 - val_acc: 0.8571\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.86214\n",
      "Epoch 7/10\n",
      " - 48s - loss: 0.2351 - acc: 0.9189 - val_loss: 0.4659 - val_acc: 0.8650\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.86214 to 0.86500, saving model to handgesture.weights.best.hdf5\n",
      "Epoch 8/10\n",
      " - 47s - loss: 0.1966 - acc: 0.9395 - val_loss: 0.4908 - val_acc: 0.8700\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.86500 to 0.87000, saving model to handgesture.weights.best.hdf5\n",
      "Epoch 9/10\n",
      " - 48s - loss: 0.1699 - acc: 0.9437 - val_loss: 0.5212 - val_acc: 0.8671\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.87000\n",
      "Epoch 10/10\n",
      " - 47s - loss: 0.1732 - acc: 0.9434 - val_loss: 0.6083 - val_acc: 0.8821\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.87000 to 0.88214, saving model to handgesture.weights.best.hdf5\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "nb_epoch = 10\n",
    "\n",
    "# Train the model\n",
    "\n",
    "hist = model.fit(\n",
    "    X_train_new,\n",
    "    y_train_new,\n",
    "    validation_data=(X_val_new,y_val_new),\n",
    "    batch_size = batch_size,\n",
    "    epochs = nb_epoch,\n",
    "    shuffle=True,\n",
    "    callbacks  = callbacks_list,\n",
    "    verbose = 2\n",
    "    )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1400/1400 [==============================] - 5s 4ms/step\n",
      "accuracy: 88.21428571428571\n"
     ]
    }
   ],
   "source": [
    "model.load_weights(\"handgesture.weights.best.hdf5\")\n",
    "\n",
    "# Evaluate the model\n",
    "score = model.evaluate(\n",
    "    X_val_new,\n",
    "    y_val_new,\n",
    "    batch_size=batch_size,\n",
    "    )\n",
    "print('accuracy:',score[1]*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250, 18, 84, 84)\n",
      "load Swiping Left data completed!\n",
      "(500, 18, 84, 84)\n",
      "load Swiping Right data completed!\n",
      "(750, 18, 84, 84)\n",
      "load Swiping Down data completed!\n",
      "(1000, 18, 84, 84)\n",
      "load Swiping Up data completed!\n",
      "(1250, 18, 84, 84)\n",
      "load Zooming In data completed!\n",
      "(1500, 18, 84, 84)\n",
      "load Zooming Out data completed!\n",
      "(1750, 18, 84, 84)\n",
      "load No Gesture data completed!\n"
     ]
    }
   ],
   "source": [
    "test_folder = 'dataset\\\\test\\\\'\n",
    "# Load training data\n",
    "X_test = []     # variable to store entire dataset\n",
    "\n",
    "#Reading data from each class\n",
    "for class_name in classes:    \n",
    "    class_folder = os.path.join(test_folder, class_name )\n",
    "    listing = os.listdir(class_folder)\n",
    "    for vid in listing:\n",
    "        frames = []\n",
    "        # set number of necessary frames\n",
    "        num_frames_necessary = img_depth * step_sample \n",
    "        vid = os.path.join(class_folder, vid)\n",
    "        frame_names = os.listdir(vid)\n",
    "        frame_names = list(sorted(frame_names))\n",
    "    #     print(frame_names)\n",
    "        num_frames = len(frame_names)\n",
    "        # pick frames\n",
    "        offset = 0\n",
    "        if num_frames_necessary > num_frames:\n",
    "            # Pad last frame if video is shorter than necessary\n",
    "            frame_names += [frame_names[-1]] * \\\n",
    "                (num_frames_necessary - num_frames)        \n",
    "        elif num_frames_necessary < num_frames:\n",
    "            # If there are more frames, then sample starting offset.\n",
    "            diff = (num_frames - num_frames_necessary)\n",
    "            offset = np.random.randint(0, diff)\n",
    "        frame_names = frame_names[offset:num_frames_necessary + offset:2]      \n",
    "        for name in frame_names:\n",
    "            name = os.path.join(vid,name)\n",
    "    #         print(name)\n",
    "            frame= cv2.imread(name)\n",
    "            frame=cv2.resize(frame,(img_rows,img_cols),interpolation=cv2.INTER_AREA)\n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            frames.append(gray)      \n",
    "        ipt = np.array(frames)     \n",
    "        X_test.append(ipt)\n",
    "    print(np.array(X_test).shape)\n",
    "    print('load {} data completed!'.format(class_name))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1750, 18, 84, 84, 1)\n",
      "(1750, 7)\n"
     ]
    }
   ],
   "source": [
    "# Assign Label to each class for test set\n",
    "num_samples = np.array(X_test).shape[0]\n",
    "label=np.ones((num_samples,),dtype = int)\n",
    "label[0:250]= 0         # swiping left\n",
    "label[250:500] = 1      # swiping right\n",
    "label[500:750] = 2     # swiping down\n",
    "label[750:1000] = 3     # swiping up\n",
    "label[1000:1250] = 4     # zooming in\n",
    "label[1250:1500] = 5     # zooming out\n",
    "label[1500:1750] = 6     # No gestures\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "X_test = np.expand_dims(X_test, axis=-1)\n",
    "\n",
    "y_test = label\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre-processing\n",
    "X_testing = X_test.astype('float32')\n",
    "X_testing  -= np.mean(X_testing )\n",
    "X_testing  /= np.max(X_testing )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1750/1750 [==============================] - 5s 3ms/step\n",
      "accuracy: 73.6\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "score = model.evaluate(\n",
    "    X_testing,\n",
    "    Y_test,\n",
    "    batch_size=batch_size,\n",
    "    )\n",
    "print('accuracy:',score[1]*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0, 243,   0,   2,   3,   2,   0],\n",
       "       [  0, 246,   1,   0,   1,   2,   0],\n",
       "       [  0,  14, 199,  19,  10,   7,   1],\n",
       "       [  0,   4,  37, 189,   9,  11,   0],\n",
       "       [  0,   4,   7,   6, 198,  32,   3],\n",
       "       [  0,   4,  10,   2,  17, 212,   5],\n",
       "       [  0,   2,   1,   1,   0,   2, 244]], dtype=int64)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Generate Confusion matrix\n",
    "import sklearn \n",
    "from sklearn.metrics import confusion_matrix\n",
    "Y_predict = model.predict_classes(X_testing)\n",
    "confusion_matrix(label,Y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imshow('input',X_test[1,1,:,:,:])\n",
    "cv2.waitKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_vid_id = 1 \n",
    "test_vid = X_test[test_vid_id]\n",
    "test_vid_t = np.expand_dims(test_vid, axis=0)\n",
    "\n",
    "predicted = int(model.predict_classes(test_vid_t))\n",
    "\n",
    "for i in range(18):\n",
    "    image = test_vid[i]\n",
    "    image = cv2.resize(image,(800,500), interpolation = cv2.INTER_CUBIC)\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    cv2.putText(image,'Predict_gesture: {}'.format(classes[predicted]),(50,50), font, 1 ,(255,0,0),1,cv2.LINE_AA)\n",
    "    cv2.imshow('input',image)\n",
    "    cv2.waitKey(500)\n",
    "    \n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:3DCNN]",
   "language": "python",
   "name": "conda-env-3DCNN-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
